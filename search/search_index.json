{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the LINC Documentation","text":"<p>The center for Large-scale Imaging of Neural Circuits (LINC)  (PIs: Haber, Hillman, Yendiki) is funded by the  NIH BRAIN Initiative CONNECTS program. Its goal is to develop novel technologies for imaging brain connections down to  the microscopic scale, and deploy these technologies to image  cortico-subcortical projections that are relevant to deep brain stimulation for  motor and psychiatric disorders.</p>"},{"location":"#about-this-doc","title":"About this doc","text":"<p>The LINC documentation is meant to share information across the LINC project investigators.  If you are new to the LINC project, you can start on the Upload data page for a description of how to interact with the LINC data sharing platform.</p> <p>Since the LINC infrastructure is a fork of the DANDI Archive, please refer to the DANDI Docs for comprehensive documentation.</p>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>LINC Homepage</li> <li>LINC Data Sharing Platform</li> <li>LINC Project Code on GitHub</li> </ul>"},{"location":"#support","title":"Support","text":"<p>For questions, bug reports, and feature requests, please:</p> <ul> <li>File an issue on the relevant GitHub repository</li> <li>Reach out on the LINC Slack</li> <li>Send an email to lincbrain@mit.edu</li> </ul>"},{"location":"about/","title":"About this doc","text":""},{"location":"about/#acknowledgements","title":"Acknowledgements","text":"<p>Thank you to the DANDI Archive project for setting up the documentation framework that is utilized here.  See the DANDI Docs for more information.</p>"},{"location":"about/#license","title":"License","text":"<p>This work is licensed under a Creative Commons Attribution 4.0 International License.</p>"},{"location":"contribute/","title":"Contribute to this documentation","text":"<p>If you find an issue with this documentation please file an issue or submit a pull request on the linc-docs repository.</p> <p>If you would like to contribute to the LINC documentation and render the documentation locally as you make edits, please follow the steps below:</p> <ol> <li>Fork the linc-docs repository and clone it to your computer.</li> <li>Set up a Python environment with the dependencies in the requirements.txt file.</li> <li>Within the Python environment, run <code>mkdocs serve</code>.  This will build the website and start a local webserver (e.g. at http://127.0.0.1:8000) with your documentation.</li> <li>As you continue to edit the markdown files or configuration file, your documentation will be automatically re-built and rendered locally.</li> <li>Commit your changes and submit a pull request.</li> </ol>"},{"location":"dashboard/","title":"LINC Dashboard","text":"<p>The LINC Dashboard summarizes the data on lincbrain.org.</p> <p>Excluded from the dashboard are datasets <code>000048</code> (OpenBNB) and <code>000004</code> (Mouse LSM) since they were not acquired as part of LINC milestones.</p>"},{"location":"dashboard/#developer-documentation","title":"Developer documentation","text":""},{"location":"dashboard/#architecture","title":"Architecture","text":"<ul> <li>The front-end is built with the Streamlit framework and deployed with Streamlit's Community Cloud.</li> <li>The metadata is extracted using a GitHub Action and the CSV files are uploaded to a private AWS S3 bucket.</li> </ul>"},{"location":"dashboard/#run-application-locally","title":"Run application locally","text":"<p>Follow the instructions below to run the application locally: 1. Fork and clone the linc-dashboard repository. 2. Install the dependencies using <code>pip install -r requirements.txt</code>. 3. Run the application with <code>streamlit run app.py</code>. 4. Run the script to extract metadata with <code>python extract_metadata.py</code>.  Note, by default the application displays the data from S3.</p>"},{"location":"engaging/","title":"MIT Engaging High Performance Compute Cluster","text":"<p>The Engaging High Performance Compute Cluster is available to LINC team members to process their jobs at scale, including with the use of GPUs.</p>"},{"location":"engaging/#create-an-account","title":"Create an account","text":"<p>In order to access the Engaging Cluster, you will need a MIT Sponsored Account.</p> <ol> <li>Please contact Kabi at kabi@mit.edu with your organization name, date of birth, and phone number.</li> <li>Once the sponsored account is approved, you will receive an email to complete account registration and establish your MIT Kerberos identity.</li> <li>Please send your Kerberos ID to Kabi so that he can add you to the WebMoira group (<code>orcd_ug_pg_linc_all</code>) so that you can access the Engaging Cluster.</li> <li>Sign up for two-factor authentication with Duo at https://duo.mit.edu.  You will need to download the Duo Mobile app and register your phone with the QR code that is displayed at https://duo.mit.edu.</li> <li>To access the Engaging cluster for the first time, you will need to log in via the Engaging Open OnDemand Portal.  For instructions, see here.</li> </ol>"},{"location":"engaging/#documentation-overview","title":"Documentation overview","text":"<p>The MIT Office of Research Computing and Data (ORCD) manage the Engaging Cluster.  Most of the information you will need is in the first link below but there are additional resources:</p> <ol> <li>Engaging Cluster docs</li> <li>ORCD Docs</li> <li>MGHPCC OpenMind GitHub wiki</li> <li>Slurm docs</li> </ol>"},{"location":"engaging/#email-updates","title":"Email updates","text":"<p>The ORCD team will regularly send out updates about the Engaging Cluster to your MIT email address, which can be accessed at https://outlook.office.com.</p>"},{"location":"engaging/#access-the-cluster-and-run-jobs","title":"Access the cluster and run jobs","text":"<p>The Engaging Cluster has head/login nodes to access the cluster and submit jobs to the compute nodes which run your resource intensive scripts.  Job orchestration is performed with the Slurm Workload Manager.  The Engaging Cluster Documentation provides details on these operations, including:</p> <ol> <li>Logging into the cluster</li> <li>Cluster architecture including information on the head/login nodes versus compute nodes</li> <li>Common commands to interact with the Slurm Job Scheduler</li> <li>Run multiple jobs in parallel with <code>sbatch</code></li> <li>Run interactive jobs on a single compute node with <code>srun</code> or <code>salloc</code></li> <li>Access installed software</li> <li>Determining resources for your job</li> </ol> <p>Slurm is a common workload manager so you can also refer to the official Slurm documentation.</p>"},{"location":"engaging/#compute-nodes","title":"Compute nodes","text":"<p>The Engaging Cluster has several CPU-only compute nodes and GPU compute nodes.  The nodes are categorized according to partitions to control which groups have access to the nodes.</p> <p>See Determining resources for your job for details on selecting the nodes and resources for your jobs.  Briefly, the <code>sinfo</code> command shows the partitions where you can submit jobs and you can submit jobs to a certain partition by including <code>#SBATCH --partition=&lt;partition_name&gt;</code> in your sbatch script.</p> <p>The GPU nodes are available through the <code>ou_bcs_high</code> and <code>ou_bcs_low</code> partitions.  For more details, see the BCS computing resources on Engaging - Slurm configuration wiki.</p>"},{"location":"engaging/#data-storage","title":"Data storage","text":"<p>Data can be stored under the following path: <code>/orcd/data/linc/</code>.  We will be working to create an organization strategy for the LINC project data but for now please store your data under a subdirectory (e.g. <code>/orcd/data/linc/&lt;username&gt;</code> or <code>/orcd/data/linc/&lt;projectname&gt;</code>).  There are additional locations to store your data including the use of scratch space (<code>/orcd/scratch/bcs/001</code>, <code>/orcd/scratch/bcs/002</code>, <code>/pool001/&lt;username&gt;</code>) which can be found under the Storage page and the BCS computing resources on Engaging wiki.</p>"},{"location":"engaging/#best-practices","title":"Best practices","text":"<ol> <li>Please be respectful of these resources as they are used by many groups.</li> <li>Only run resource intensive scripts on the compute nodes and not on the login/head nodes.</li> <li>Only run the steps in your script on a GPU compute node if those steps require a GPU.  All other steps should be run on a CPU-only compute node.</li> <li>Monitor your jobs frequently (<code>squeue -u &lt;username&gt;</code>).</li> </ol>"},{"location":"jupyterhub/","title":"JupyterHub","text":"<p>All LINC users have access to the DANDI Hub.</p>"},{"location":"jupyterhub/#on-demand-instances","title":"On-demand instances","text":"<p>Currently on DANDI Hub, all users have access to spot instances, which are cheaper but the sessions could get interrupted. Please let Kabi know if you are running long processing sessions and would like access to the on-demand instances.</p> <p>Once you have access to the on-demand instances, select <code>LINC Instance</code> from the <code>Server Options</code>. See image below.</p> <p></p> <p>Once you are done with the instance, navigate to https://hub.dandiarchive.org/hub/home and please select <code>Stop My Server</code>. There is an automatic timeout but this will help save costs.</p> <p></p>"},{"location":"neuroglancer-private-assets/","title":"Visualize private S3 assets with Neuroglancer","text":"<p>Collaborators: Aaron Kanzer, Kabilar Gunalan</p> <p>Outlined below are 3 use cases for visualizing private assets in Neuroglancer. In the current effort, requirements and a subsequent implementation will be developed for use cases 1 and 2.</p>"},{"location":"neuroglancer-private-assets/#use-cases","title":"Use cases","text":""},{"location":"neuroglancer-private-assets/#use-case-1","title":"Use case 1","text":"<p>Use the <code>External Services</code> button in the <code>FileBrowser</code> to load a Neuroglancer viewer with a single layer of the corresponding asset.  The resulting URL can be shared between LINC team members.  This use case extends the capabilities in the DANDI Archive to view assets in a private S3 bucket.</p>"},{"location":"neuroglancer-private-assets/#use-case-2","title":"Use case 2","text":"<p>Set a single Neuroglancer viewer programmatically to visualize multiple assets (e.g. dMRI, HiP-CT) as layers.</p>"},{"location":"neuroglancer-private-assets/#use-case-3","title":"Use case 3","text":"<p>Use case 2 + Load the tractography-generated streamlines (i.e. <code>trk</code> file) into memory, convert the streamlines into the Neuroglancer skeleton format, and visualize in the Neuroglancer viewer as an additional layer.</p>"},{"location":"neuroglancer-private-assets/#requirements","title":"Requirements","text":"<ol> <li>Zarr archives are stored in a private AWS S3 bucket</li> <li>Provide LINC users with direct access to the assets that are stored in the private S3 bucket using a URI.</li> </ol>"},{"location":"neuroglancer-private-assets/#implementation-options","title":"Implementation Options","text":""},{"location":"neuroglancer-private-assets/#issue-read-only-credentials-for-s3-bucket-quick-set-up","title":"Issue read-only credentials for S3 bucket. Quick set up.","text":"<p>Does not resolve rendering issue. Would resolve accessing the asset in Jupyter Notebook, but would require flaky Python code.</p>"},{"location":"neuroglancer-private-assets/#integrate-read-only-credentials-for-s3-bucket-into-linc-archive-linc-cli","title":"Integrate read-only credentials for S3 bucket into LINC Archive, LINC CLI","text":""},{"location":"neuroglancer-private-assets/#cloudfront-distribution-with-origin-access-identity-control-for-relevant-s3-buckets","title":"CloudFront distribution with Origin Access Identity control for relevant S3 buckets","text":"<ul> <li>S3 Asset Bucket is served via a CloudFront distribution</li> <li>User navigates to LINC Archive (i.e. <code>lincbrain.org</code>) to authenticate.  Thereby the user hits a get_presigned_cookie/ API endpoint. In the endpoint response is a valid cookie that allows the user's browser to be able to access the asset via their browser.</li> </ul> <p>Render with hosted Neuroglancer - Host Neuroglancer fork on S3 bucket with static site hosting enabled. Create an AWS CloudFront distribution to sit as a verified proxy with permission to access many S3 assets at once via presigned cookies.  Provide CNAME for CloudFront Distribution URL to <code>neuroglancer.lincbrain.org</code> so that cookies generated by LINC Archive (also living at a <code>*.lincbrain.org</code>) can be shared between the API and the Neuroglancer site. - Modify the Neuroglancer source code to handle a CloudFront asset path (i.e. <code>neuroglancer.lincbrain.org</code> since the domains of the presigned cookie are limited to <code>*.lincbrain.org</code> domains due to same-site requirements for cookies) that is similar to any S3 asset path (i.e. <code>s3://</code>). - Cookies would get passed in request to get asset - See the diagram section for further details on the design.</p>"},{"location":"neuroglancer-private-assets/#aws-cognito-npm-aws-sdk-package","title":"AWS Cognito, npm AWS SDK package","text":"<p>TBD</p>"},{"location":"neuroglancer-private-assets/#provide-endpoint-to-provide-pre-signed-s3-asset-urls","title":"Provide endpoint to provide pre-signed S3 Asset URLs","text":"<p>Blocker: Pre-signed URLs can only be generated at the object-level, not at the sub-directory level. Neuroglancer renders many objects at once as a user zooms, scrolls, etc., thus unless we generated a pre-signed URL for each asset, this would be difficult.</p>"},{"location":"neuroglancer-private-assets/#netlify-oauth-requirement-to-render-site","title":"Netlify OAuth requirement to render site","text":"<p>Would pass credentials to AWS.  Solves rendering issue, but does not solve accessing private S3 assets.</p>"},{"location":"neuroglancer-private-assets/#diagram","title":"Diagram","text":"<pre><code>%%{init: {\"flowchart\": {\"curve\": \"linear\"}}}%%\nflowchart LR\n    A(User) --&gt; B{Does client have CloudFront cookies from a prior session?}\n    B -- No --&gt; C(LINC Archive API &lt;br/&gt; GET request /api/permissions/s3/)\n    B -- Yes --&gt; D(Static Neuroglancer Webpage)\n    C --&gt; D\n    D -- Send presigned cookies --&gt; E(AWS CloudFront)\n    E --&gt; F(Private AWS S3 Bucket)\n    F -- Neuroglancer able to access S3 data and render on screen. --&gt; D</code></pre>"},{"location":"neuroglancer/","title":"Using Neuroglancer to Visualize Orientation Vector in Zarr Format","text":""},{"location":"neuroglancer/#1-ensure-the-vector-dimension-is-in-a-single-chunk","title":"1. Ensure the vector dimension is in a single chunk","text":"<p>Neuroglancer requires that the dimension along the channels (where your vector data resides) must be within a single chunk. Specifically, the chunk size along the channel dimension must be 3. </p>"},{"location":"neuroglancer/#2-load-the-zarr-data-into-neuroglancer","title":"2. Load the Zarr data into Neuroglancer","text":"<p>Once you have your data chunked correctly, you can load the Zarr dataset into Neuroglancer. Typically, you will:</p>"},{"location":"neuroglancer/#21-start-local-neuroglancer-server","title":"2.1 Start Local Neuroglancer Server","text":"<ol> <li>Start Neuroglancer.</li> <li>Open Neuroglancer in a browser.</li> <li>Load Zarr if it presents locally  <code>load zarr:///example.path.zarr</code></li> </ol>"},{"location":"neuroglancer/#22-start-neuroglancer-on-lincbrain","title":"2.2 Start Neuroglancer on lincbrain","text":"<ol> <li>Find the dataset you want to visualize</li> <li>Under <code>Open With</code> button, select Neuroglancer</li> </ol>"},{"location":"neuroglancer/#3-rename-the-channel-dimension-from-c-to-c","title":"3. Rename the channel dimension from <code>c'</code> to <code>c^</code>","text":"<p>Neuroglancer may label your dimension as <code>c'</code> or something else automatically.  </p> <p>Renaming this dimension helps Neuroglancer interpret the data as a vector field (dimension of 3).  </p>"},{"location":"neuroglancer/#4-apply-a-custom-shader-for-visualization","title":"4. Apply a custom shader for visualization","text":"<p>To actually see your vector data meaningfully rendered, you need a custom shader. Neuroglancer supports a small GLSL-like language for defining how each voxel is colored.</p>"},{"location":"neuroglancer/#41-if-the-vectors-already-have-unit-norm","title":"4.1 If the vectors already have unit norm","text":"<p>If you know that each voxel\u2019s vector is already normalized (i.e., \\(\\|\\mathbf{v}\\| = 1\\)), you can use the following simple shader to visualize the absolute value of each component (as RGB channels):</p> <pre><code>void main() {\n  vec3 color;\n  color.r = abs(getDataValue(0));\n  color.g = abs(getDataValue(1));\n  color.b = abs(getDataValue(2));\n  emitRGB(color);\n}\n</code></pre>"},{"location":"neuroglancer/#42-if-the-vectors-do-not-have-unit-norm","title":"4.2 If the vectors do not have unit norm","text":"<p>If your vectors do not have unit norm, and the magnitude of each vector encodes additional information (like reliability of the measurement), you can normalize the color in the shader and use the magnitude as alpha:</p> <pre><code>void main() {\n  vec4 color;\n  color.r = abs(getDataValue(0));\n  color.g = abs(getDataValue(1));\n  color.b = abs(getDataValue(2));\n\n  // Compute the norm (magnitude) of the vector\n  color.a = sqrt(color.r * color.r + color.g * color.g + color.b * color.b);\n\n  // Normalize the color by the magnitude\n  color.r = color.r / color.a;\n  color.g = color.g / color.a;\n  color.b = color.b / color.a;\n\n  // Scale alpha by some maximum norm if desired. If you have a known maximum,\n  // replace MAX_NORM with that value; otherwise you can leave the alpha\n  // as-is or adjust as needed.\n  float MAX_NORM = 1.0; // modify this as appropriate\n  color.a = color.a / MAX_NORM;\n\n  emitRGBA(color);\n}\n</code></pre> <p>In Neuroglancer, you can paste this shader code in the layer\u2019s Shader Editor (usually found under the \u201cLayer\u201d panel).</p>"},{"location":"neuroglancer/#5-example","title":"5. Example","text":"<p>An example dataset can be found at here. The file <code>sample18_st_filtered.ome.zarr</code>. Once we change the dimension name to <code>c^</code> and apply the shader. We can see the result as following.  </p>"},{"location":"upload/","title":"Uploading data to lincbrain.org","text":"<p>Note: The steps below require access to https://lincbrain.org, which is a private repository for LINC project investigators.</p>"},{"location":"upload/#contribute-to-an-existing-dataset-or-create-a-new-dataset","title":"Contribute to an existing dataset or create a new dataset","text":"<p>Before you create a new dataset, please look over the existing datasets and make sure that there is no dataset appropriate for adding your new images to. Only create a new dataset as a last resort. If your images come from a brain that is already included in an existing dataset, definitely add to the existing dataset, even if the existing images come from a different part of that brain and/or were imaged with a different modality than yours. A dataset refers to a collection of brains that have been processed and imaged for a similar purpose. It typically contains data from multiple brains or samples, possibly imaged with multiple modalities.</p> <ol> <li>Contribute to an existing dataset<ol> <li>Log into the LINC Data Platform.</li> <li>Browse the existing datasets under the SHARED DATASETS tab.</li> <li>Contact the <code>Owner</code> by email requesting that they add you as an <code>Owner</code> of the dataset.</li> </ol> </li> <li>Create a new dataset<ol> <li>Log into the LINC Data Platform.</li> <li>Browse the existing datasets under the SHARED DATASETS tab. Make sure that you have followed the guidelines above and there is no existing dataset that your new files can be added to.</li> <li>Click on the <code>NEW DATASET</code> button at the top right of the page.</li> <li>Fill out the title, description, and license. (The license option exists only because lincbrain.org is a clone of DANDI. It has no effect here, as lincbrain.org is a private repository.)</li> <li>Click on the <code>REGISTER DATASET</code> button to create the new dataset.</li> </ol> </li> </ol>"},{"location":"upload/#install-the-dandi-command-line-interface-cli","title":"Install the dandi command-line interface (CLI)","text":"<p>On your local machine, install the dandi CLI package in a python environment:</p> <p><code>pip install dandi</code></p> <p>Note: If you are in the MGH LINC team, do not do this step. Instead, ask your labmates how to activate the existing dandi python environment.</p>"},{"location":"upload/#copy-your-lincbrain-api-key","title":"Copy your lincbrain API key","text":"<p>Log into lincbrain.org and click on the button with your initials at the top right of the page. Copy your API key and enter it in the following environment variable on your local machine:</p> <p><code>export DANDI_API_KEY=&lt;EnterYourKeyHere&gt;</code></p>"},{"location":"upload/#download-your-new-empty-dataset-locally","title":"Download your new (empty) dataset locally","text":""},{"location":"upload/#for-a-new-dataset","title":"For a new dataset","text":"<p>You can find the command that you need to run to download a specific dataset by navigating to the dataset landing page on lincbrain.org, clicking on the <code>DOWNLOAD</code> drop-down menu that you'll see at the top right corner of that page, and copying the <code>dandi download ...</code> command that you see when you click on that menu. </p> <p>On your local machine, create a directory that you will use as a staging area for uploading data. Then change into this directory, and run the download command that you copied above. For example: <pre><code>cd /path/to/my/staging/area\ndandi download https://lincbrain.org/dandiset/101010/draft\n</code></pre></p> <p>The above example will create a directory called <code>/path/to/my/staging/area/101010</code> with a file called <code>dandiset.yaml</code> in it. Any data files that you want to upload to your new lincbrain.org dataset have to first be saved here, and organized according to the Brain Imaging Data Structure (BIDS).</p>"},{"location":"upload/#for-an-existing-dataset","title":"For an existing dataset","text":"<p>For an existing dataset you will probably not want to download the entire dataset from lincbrain.org which can be many terabyes in size.  Using the command below, you can download the dataset with just the <code>dandiset.yaml</code> and <code>dataset_description.json</code> files.  You will need to replace <code>101010</code> with your <code>dandiset-id</code>.</p> <pre><code>cd /path/to/my/staging/area\ndandi download --preserve-tree dandi://linc/101010@draft/dataset_description.json\n</code></pre> <p>The above example will create a directory called <code>/path/to/my/staging/area/101010</code>. Any data files that you want to upload to the lincbrain.org dataset have to first be saved here, and organized according to the Brain Imaging Data Structure (BIDS).</p> <p>For more information on the <code>--preserve-tree</code> option please see the DANDI Docs.</p>"},{"location":"upload/#organize-your-data","title":"Organize your data","text":"<p>An example from a (fictional) dataset that includes dMRI and histology data from two brains is shown below. This can be used as a guide for organizing your own dataset. A few things to watch out for: 1. If you are creating a new dataset, you have to create the <code>dataset_description.json</code> file. 1. If you are adding data from a new subject to an existing dataset, you have to add an entry with the new subject's information to the <code>participants.tsv</code> file. 1. If you are adding data from a new sample (part of the brain) to an existing dataset, you have to add an entry with the new sample's information to the <code>samples.tsv</code> file. 1. Any data files that you add must go under:    * <code>rawdata/</code> if they are raw (unprocessed) data in NIfTI or Zarr format    * <code>sourcedata/</code> if they are raw (unprocessed) data in other formats    * <code>derivatives/</code> if they are the outputs of pre-processing or other downstream analyses of the raw data </p> <pre><code>101010/\n  dataset_description.json\n  participants.tsv\n  samples.tsv\n  rawdata/\n    sub-Ken1/\n      dwi/\n        sub-Ken1_sample-brain_acq-DSI_dwi.bval\n        sub-Ken1_sample-brain_acq-DSI_dwi.bvec\n        sub-Ken1_sample-brain_acq-DSI_dwi.json\n        sub-Ken1_sample-brain_acq-DSI_dwi.nii.gz\n      micr/\n        sub-Ken1_sample-slice0001_photo.json\n        sub-Ken1_sample-slice0001_photo.tif\n        sub-Ken1_sample-slice0001_stain-Nissl_BF.json\n        sub-Ken1_sample-slice0001_stain-Nissl_BF.tif\n        sub-Ken1_sample-slice0002_photo.json\n        sub-Ken1_sample-slice0002_photo.tif\n        sub-Ken1_sample-slice0002_stain-LY_DF.json\n        sub-Ken1_sample-slice0002_stain-LY_DF.tif\n        sub-Ken1_sample-slice0009_photo.json\n        sub-Ken1_sample-slice0009_photo.tif\n        sub-Ken1_sample-slice0009_stain-Nissl_BF.json\n        sub-Ken1_sample-slice0009_stain-Nissl_BF.tif\n        sub-Ken1_sample-slice0010_photo.json\n        sub-Ken1_sample-slice0010_photo.tif\n        sub-Ken1_sample-slice0010_stain-LY_DF.json\n        sub-Ken1_sample-slice0010_stain-LY_DF.tif\n    sub-Ken2/  \n      dwi/\n        sub-Ken2_sample-brain_acq-DSI_dwi.bval\n        sub-Ken2_sample-brain_acq-DSI_dwi.bvec\n        sub-Ken2_sample-brain_acq-DSI_dwi.json\n        sub-Ken2_sample-brain_acq-DSI_dwi.nii.gz\n        sub-Ken2_sample-brain_acq-MulShellMulTE_dwi.bval\n        sub-Ken2_sample-brain_acq-MulShellMulTE_dwi.bvec\n        sub-Ken2_sample-brain_acq-MulShellMulTE_dwi.json\n        sub-Ken2_sample-brain_acq-MulShellMulTE_dwi.nii.gz\n      micr/\n        sub-Ken2_sample-slice0001_photo.json\n        sub-Ken2_sample-slice0001_photo.tif\n        sub-Ken2_sample-slice0001_stain-Nissl_BF.json\n        sub-Ken2_sample-slice0001_stain-Nissl_BF.tif\n        sub-Ken2_sample-slice0002_photo.json\n        sub-Ken2_sample-slice0002_photo.tif\n        sub-Ken2_sample-slice0002_stain-LY_DF.json\n        sub-Ken2_sample-slice0002_stain-LY_DF.tif\n        sub-Ken2_sample-slice0003_photo.json\n        sub-Ken2_sample-slice0003_photo.tif\n        sub-Ken2_sample-slice0003_stain-FR_DF.json\n        sub-Ken2_sample-slice0003_stain-FR_DF.tif\n        sub-Ken2_sample-slice0009_photo.json\n        sub-Ken2_sample-slice0009_photo.tif\n        sub-Ken2_sample-slice0009_stain-Nissl_BF.json\n        sub-Ken2_sample-slice0009_stain-Nissl_BF.tif\n        sub-Ken2_sample-slice0010_photo.json\n        sub-Ken2_sample-slice0010_photo.tif\n        sub-Ken2_sample-slice0010_stain-LY_DF.json\n        sub-Ken2_sample-slice0010_stain-LY_DF.tif\n        sub-Ken2_sample-slice0011_photo.json\n        sub-Ken2_sample-slice0011_photo.tif\n        sub-Ken2_sample-slice0011_stain-FR_DF.json\n        sub-Ken2_sample-slice0011_stain-FR_DF.tif\n  derivatives/\n    sub-Ken1/\n      dwi/\n        ...\n      scenes/\n        ...\n      xfms/\n        ...\n    sub-Ken2/\n      dwi/\n        ...\n      scenes/\n        ...\n      xfms/\n        ...\n</code></pre> <p>The files and subdirectories in this example dataset are described in detail below.</p>"},{"location":"upload/#top-level-files","title":"Top-level files","text":""},{"location":"upload/#dataset_descriptionjson","title":"dataset_description.json","text":"<p>This text file is described in detail in the BIDS specification. A minimal <code>dataset_description.json</code> would look like this:</p> <pre><code>{\n    \"Name\": \"Seminal post mortem dMRI and histology dataset from the laboratory of Dr. Barbara Millicent Roberts\",\n    \"BIDSVersion\": \"1.9.0\"\n}\n</code></pre>"},{"location":"upload/#participantstsv","title":"participants.tsv","text":"<p>This text file is described in detail in the BIDS specification. For this dataset, the <code>participants.tsv</code> might look like this:</p> <pre><code>participant_id age sex diagnosis\nsub-Ken1 43 M healthy\nsub-Ken2 61 M hypertension\n</code></pre>"},{"location":"upload/#samplestsv","title":"samples.tsv","text":"<p>This text file is described in detail in the BIDS specification. For this dataset, the <code>samples.tsv</code> would look like this:</p> <pre><code>participant_id sample_id sample_type\nsub-Ken1 sample-brain tissue\nsub-Ken1 sample-slice0001 tissue\nsub-Ken1 sample-slice0002 tissue\nsub-Ken1 sample-slice0009 tissue\nsub-Ken1 sample-slice0010 tissue\nsub-Ken2 sample-brain tissue\nsub-Ken2 sample-slice0001 tissue\nsub-Ken2 sample-slice0002 tissue\nsub-Ken2 sample-slice0003 tissue\nsub-Ken2 sample-slice0009 tissue\nsub-Ken2 sample-slice0010 tissue\nsub-Ken2 sample-slice0011 tissue\n</code></pre> <p>Label the sample as <code>brain</code> if the whole brain was imaged. Label it as <code>lefthemi</code> or <code>righthemi</code> if a whole hemisphere was imaged.</p>"},{"location":"upload/#rawdata","title":"rawdata/","text":"<p>This directory contains one subdirectory for each brain, which contains one subdirectory for each modality, which in turn contains raw image data files named according to the BIDS specification.</p> <p>The name of every image or other data file under <code>rawdata/</code> must start with a subject and sample label, in that order. Examples:  * <code>sub-Ken1_sample-brain_*.nii.gz</code>: NIfTI images of the whole brain of subject Ken1 * <code>sub-Ken2_sample-slice0001_*.json</code>: metadata files associated with images of a section of subject Ken2</p>"},{"location":"upload/#dwi","title":"dwi/","text":"<p>This directory contains dMRI data files as described in detail in the BIDS specification. </p> <p>In this example the data include images (<code>.nii.gz</code>), b-value tables (<code>.bval</code>), gradient tables (<code>.bvec</code>), and metadata (<code>.json</code>). The name of every image or other data file under <code>rawdata/dwi/</code> must contain the subject, sample, and acquisition label, in that order, followed by the <code>dwi</code> suffix. Examples: * <code>sub-Ken1_sample-brain_acq-DSI_dwi.nii.gz</code>: NIfTI volumes of DWIs acquired with a DSI scheme from the whole brain of subject Ken1 * <code>sub-Ken2_sample-brain_acq-MulShellMulTE_dwi.bval</code>: b-value table of a scan acquired with a multi-shell, multi-echo scheme, from the whole brain of subject Ken2</p>"},{"location":"upload/#micr","title":"micr/","text":"<p>This directory contains microscopy data files as described in detail in the BIDS specification.</p> <p>In this example the data include images (<code>.tif</code>) and metadata (<code>.json</code>) from multiple brain sections. For each section there is a blockface photo (<code>_photo</code>) and a histological stain (<code>_stain</code>). Sections from Ken1 and Ken2 were either processed with a Nissl stain and imaged under brightfield microscopy (<code>_BF</code>), or processed for the fluorescent tracer Lucifer Yellow (<code>LY</code>) and imaged under darkfield microscopy (<code>_DF</code>). Additional sections from Ken2 were processed for the fluorescent tracer Fluoro-Ruby (<code>FR</code>) and imaged under darkfield microscopy (<code>_DF</code>).</p>"},{"location":"upload/#derivatives","title":"derivatives/","text":"<p>This directory contains one subdirectory for each brain, which contains one subdirectory for each modality, which in turn contains any data files that were produced by analyzing the raw data of that modality. At the same level as the modality-specific directories, there can also be directories for saving files that are typically produced by co-analyzing images of more than one modality, e.g., <code>xfms/</code> for transform files or <code>scenes/</code> for scene files.</p> <p>The name of every image or data file under <code>derivatives/</code> must start its subject and sample label, in that order. For any files that have been transformed to a space other than the native space, i.e., the one where the images were acquired, this must be followed by a space label. Do not use a space label for files that are in the native space. The name of the file must end with a description label that includes the methodology followed by the name of the derivative. Examples: * <code>sub-Ken1_sample-brain_desc-DTI_FA.nii.gz</code>: a fractional anisotropy (FA) map derived from a diffusion tensor imaging (DTI) analysis of a whole-brain scan of subject Ken1, in its native (individual DWI) space * <code>sub-Ken2_sample-brain_space-CIT168_desc-CSD_tractography.trk</code>: a tractogram derived from a constrained spherical deconvolution (CSD) analysis of a whole-brain scan of subject Ken2, transformed to CIT168 template space</p>"},{"location":"upload/#special-case-high-res-histology-annotation","title":"Special case: high-res histology annotation","text":"<p>All annotation files use the following naming scheme: <code>&lt;dataset-name&gt; + _desc-[label] + _suffix.ome.zarr</code> where <code>[label]</code> is replaced by the annotator's initials and <code>_suffix</code> indicates the type of segmentations being annotated. Specifically, when annotating discrete segmentations, use <code>_dseg</code> as the suffix (see the BIDS spec on discrete segmentations).</p> <p>For example, an annotator with an initial JS annotating discrete segments would name the annotation file as <code>000003_sub-MR243_sample-slice0000slice0004_stain-LY_DF_desc-JS_dseg.ome.zarr</code></p> <p>A <code>&lt;matches&gt;.tsv</code> file could be included to map the IDs (integer values) of the discrete segmentations to the custom labels, where <code>&lt;matches&gt;</code> is replaced by the name of the annotation file. It contains a lookup table with the following columns (see the BIDS spec on custom TSV): <pre><code>index  name\n1      Single Fiber\n2      Light Bundle\n3      Moderate Bundle\n...\n</code></pre></p>"},{"location":"upload/#upload-your-data","title":"Upload your data","text":"<p>If you are uploading to a new or existing dataset on lincbrain.org, please ensure that you have the <code>dandiset.yaml</code> and <code>dataset_description.json</code> files in your local directory (i.e. the <code>/path/to/my/staging/area/101010</code> directory in the example above).</p> <p>Upload the data from your local machine to lincbrain.org:</p> <pre><code>cd /path/to/my/staging/area/101010\ndandi upload -i linc\n</code></pre> <p>Check the output in your terminal for validation errors. If there were no errors, your data files should now appear on lincbrain.org.</p>"},{"location":"upload/#delete-data-files-or-directories","title":"Delete data files or directories","text":"<p>Individual data files can be deleted on lincbrain.org by clicking on the trashcan icon next to each file. Alternatively, directories or files can be deleted from the command line.</p> <p>The following examples delete a directory named \"horses\" on lincbrain.org:</p> <pre><code>dandi delete -i linc /path/to/my/staging/area/101010/rawdata/Ken2/horses\n</code></pre> <pre><code>dandi delete -i linc \"https://lincbrain.org/dandiset/101010/draft/files?location=rawdata%2Fsub-Ken2%2Fhorses\"\n</code></pre>"},{"location":"webknossos-add-dataset/","title":"Add Webknossos Dataset","text":"<p>The following steps provide instructions for adding assets from the LINC Data Platform to LINC Webknossos.  This process has not been automated as it is often the case that multiple assets are added as layers to a single Webknossos dataset, and a pattern for adding these layers has not yet been established.</p> <ol> <li> <p>Navigate to an asset on the LINC Data Platform, and find the \u201cCopy\u201d button:</p> <p></p> </li> <li> <p>Select the AWS S3 URI \u201cCopy\u201d button:</p> <p></p> </li> <li> <p>The AWS S3 URI is now copied to your clipboard.</p> </li> <li> <p>Proceed to LINC Webknossos.</p> </li> <li> <p>Under the \u201cDatasets\u201d tab, select \u201c+ Add Dataset\u201d:</p> <p></p> </li> <li> <p>Navigate to the \u201cAdd Remote Dataset\u201d tab:</p> <p></p> </li> <li> <p>Paste the AWS S3 URI in the \u201cDataset URL\u201d field and select \u201cAdd Layer\u201d.</p> </li> <li> <p>Add the dataset name that follows the convention below:</p> <p></p> <ul> <li> <p>For a Webknossos dataset with a single layer, the name should match the file naming convention on the LINC Data Platform and add a prefix with the dataset identifier (i.e. <code>&lt;dataset-id&gt;_&lt;filename&gt;.ome.zarr</code>).  For example, the last Zarr asset in the figure below would be named as follows: <code>000003_sub-MN115_sample-slice0000slice0025_stain-LY_DF.ome.zarr</code></p> <p></p> </li> <li> <p>For a Webknossos dataset with multiple layers that are comprised of the histology image and Vaanathi Sundaresan's previously generated annotations, the name will follow the convention for Webknossos annotations and include Vaanathi Sundaresan's initials.</p> </li> </ul> </li> </ol>"},{"location":"webknossos-deployment/","title":"WEBKNOSSOS Deployment","text":"<p>This document is designed to help deploy a new version of WEBKNOSSOS via AWS EC2.</p>"},{"location":"webknossos-deployment/#create-a-new-deployment","title":"Create a new deployment","text":""},{"location":"webknossos-deployment/#create-an-instance-in-aws-ec2-with-at-least-32gb-of-memory","title":"Create an instance in AWS EC2 with at least 32GB of memory","text":"<ul> <li>Proceed to AWS and create an AWS Linux instance</li> <li>r5.2xlarge is suggested for instance type</li> <li>x86_64 architecture is suggested</li> <li>Ensure that ports 80 and 443 are available.</li> <li>Ensure that the instance is reachable via Public IP address</li> </ul>"},{"location":"webknossos-deployment/#connect-the-instance-to-a-route-53-domain-record","title":"Connect the instance to a Route 53 Domain Record","text":"<p>Proceed to Route 53 and create an A Record with the desired domain that is pointing to the Public IP address of the EC2 Instance</p>"},{"location":"webknossos-deployment/#return-to-aws-ec2-and-ssh-onto-the-instance","title":"Return to AWS EC2 and ssh onto the instance","text":"<p>Once the instance is running, SSH onto the instance. </p> <p>First, install the appropriate dependencies (i.e. docker, docker-compose, git, vim).</p> <pre><code>sudo yum install docker git vim -y\n\nsudo service docker start\n\nsudo curl -L \"https://github.com/docker/compose/releases/download/$(curl -s https://api.github.com/repos/docker/compose/releases/latest | grep -oP '\"tag_name\": \"\\K(.*)(?=\")')/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose\n\nsudo chmod +x /usr/local/bin/docker-compose\n</code></pre> <p>Next, proceed to do the following commands (These steps are mostly inline with https://docs.webknossos.org/webknossos/installation.html)</p> <pre><code>sudo mkdir opt &amp;&amp; sudo cd opt\nsudo mkdir webknossos &amp;&amp; sudo cd webknossos\n\nsudo mkdir certs &amp;&amp; sudo mkdir certs-data\n\nsudo wget https://github.com/scalableminds/webknossos/raw/master/tools/hosting/docker-compose.yml\n\nsudo mkdir binaryData\n\nsudo chown -R 1000:1000 binaryData\n\nsudo touch nginx.conf\n</code></pre> <p>Next, you'll need to issue an SSL certificate directly on the server. <code>certbot</code> is used here.</p> <pre><code>sudo docker run --rm -p 80:80 -v $(pwd)/certs:/etc/letsencrypt -v $(pwd)/certs-data:/data/letsencrypt certbot/certbot certonly --standalone -d &lt;enter-your-website-url&gt; --email admin@lincbrain.org --agree-tos --non-interactive\n</code></pre> <p>You'll need to populate the <code>nginx.conf</code>. Replace <code>&lt;enter-your-website-url&gt;</code> with the <code>A</code> name record you used in Route 53 (e.g. <code>webknossos.lincbrain.org</code>).</p> <pre><code>events {}\n\nhttp {\n    # Main server block for the webknossos application\n    server {\n        listen 80;\n        server_name &lt;enter-your-website-url&gt;;\n\n        location /.well-known/acme-challenge/ {\n            root /data/letsencrypt;\n        }\n\n        location / {\n            return 301 https://$host$request_uri;\n        }\n    }\n\n    server {\n        listen 443 ssl http2;\n        server_name &lt;enter-your-website-url&gt;;\n\n        ssl_certificate /etc/letsencrypt/live/&lt;enter-your-website-url&gt;/fullchain.pem;\n        ssl_certificate_key /etc/letsencrypt/live/&lt;enter-your-website-url&gt;/privkey.pem;\n\n        # webknossos-specific overrides\n        client_max_body_size 0;\n        proxy_read_timeout 3600s;\n\n        location / {\n            set $cors '';\n            if ($http_origin ~* (https://staging--lincbrain-org\\.netlify\\.app|https://.*\\.lincbrain\\.org|https://lincbrain\\.org)) {\n                set $cors 'true';\n            }\n\n            if ($cors = 'true') {\n                add_header 'Access-Control-Allow-Origin' \"$http_origin\" always;\n                add_header 'Access-Control-Allow-Credentials' 'true' always;\n                add_header 'Access-Control-Allow-Methods' 'GET, POST, PUT, DELETE, OPTIONS' always;\n                add_header 'Access-Control-Allow-Headers' 'Accept, Content-Type, X-Requested-With, Authorization, Cookie' always;\n            }\n\n            if ($request_method = 'OPTIONS') {\n                add_header 'Access-Control-Allow-Origin' \"$http_origin\" always;\n                add_header 'Access-Control-Allow-Credentials' 'true' always;\n                add_header 'Access-Control-Allow-Methods' 'GET, POST, PUT, DELETE, OPTIONS' always;\n                add_header 'Access-Control-Allow-Headers' 'Accept, Content-Type, X-Requested-With, Authorization, Cookie' always;\n                add_header 'Content-Length' 0 always;\n                add_header 'Content-Type' 'text/plain' always;\n                return 204;\n            }\n\n            proxy_pass http://webknossos-webknossos-1:9000;\n            proxy_http_version 1.1;\n            proxy_set_header Host $host;\n            proxy_set_header X-Real-IP $remote_addr;\n            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n            proxy_set_header X-Forwarded-Proto $scheme;\n            proxy_set_header Cookie $http_cookie;\n            proxy_set_header Transfer-Encoding \"\";\n            proxy_buffering off;\n\n            proxy_hide_header Access-Control-Allow-Origin;\n            proxy_hide_header Access-Control-Allow-Credentials;\n            proxy_hide_header Access-Control-Allow-Methods;\n            proxy_hide_header Access-Control-Allow-Headers;\n        }\n    }\n\n    # Separate server block for serving the binaryData directory\n    server {\n        listen 8080;\n        server_name &lt;enter-your-website-url&gt;;\n\n        location /binaryData/ {\n        alias /home/ec2-user/opt/webknossos/binaryData/;\n            autoindex on;\n            autoindex_exact_size off;\n            autoindex_localtime on;\n            allow all;\n        }\n    }\n}\n</code></pre> <p>You'll next want to alter the <code>docker-compose</code> pulled earlier via <code>wget</code></p> <p>Remove the <code>nginx-letsencrypt</code> service, and alter the <code>nginx</code> as such:</p> <pre><code>nginx-proxy:\n  image: nginx:latest\n  container_name: nginx-proxy\n  ports:\n    - \"8080:8080\"\n    - \"80:80\"\n    - \"443:443\"\n  volumes:\n    - ./nginx.conf:/etc/nginx/nginx.conf:ro\n    - ./certs:/etc/letsencrypt\n    - /home/ec2-user/opt/webknossos/binaryData:/home/ec2-user/opt/webknossos/binaryData:ro\n  depends_on:\n    - webknossos\n</code></pre> <p><code>nginx</code> should now be able to be called appropriately via HTTPS once <code>webknossos</code> API is running</p> <p>Lastly, you'll want to start the API and supporting containers:</p> <pre><code>DOCKER_TAG=xx.yy.z PUBLIC_HOST=webknossos.example.com LETSENCRYPT_EMAIL=admin@example.com \\\ndocker compose up -d webknossos nginx\n</code></pre> <p>You can check the health of the containers via:</p> <pre><code>docker ps\n\n# or\n\ndocker logs -f &lt;container-id&gt;\n</code></pre>"},{"location":"webknossos-deployment/#backups","title":"Backups","text":""},{"location":"webknossos-deployment/#fossildb","title":"FossilDB","text":"<p>FossilDB is a scalableminds database that extends from the open-source RocksDB.</p> <p>Temp steps / commands for FossilDB backup:</p> <ol> <li>Exec into EC2 instance</li> <li>Grab <code>fossildb-client</code> via <code>docker pull scalableminds/fossildb-client:master</code></li> <li>Determine the appropriate internal network that the <code>fossildb</code> instance is running in within the Dockerized setup on EC2: <code>docker inspect -f '{{range .NetworkSettings.Networks}}{{.NetworkID}} {{end}}' webknossos-fossildb-1</code></li> <li><code>docker run --network &lt;network-id&gt; scalableminds/fossildb-client:master webknossos-fossildb-1 backup</code> should create the backup</li> <li>The backup will be stored via <code>/home/ec2-user/opt/webknossos/persistent/fossildb/backup</code></li> </ol>"},{"location":"webknossos-deployment/#create-a-new-webknossos-with-pre-existing-backups","title":"Create a new WEBKNOSSOS with pre-existing backups","text":"<p>There are three different components that must be taken into account for a WEBKNOSSOS clone:</p> <p>\u2022 mounted Docker volumes -- represented by the <code>binaryData</code> and <code>persistent</code> directories in the WEBKNOSSOS file structure   - exported to AWS S3 via the <code>docker_volumes_backup.sh</code> cronjob script \u2022 FossilDB data (managed via <code>fossildb-client restore</code> commands)   - exported to AWS S3 via the <code>fossil_db_backup.sh</code> cronjob script \u2022 PostgresDB data (managed via <code>pg_dump</code> and <code>pg_restore</code> commands)   - exported to AWS S3 via the <code>postgres_backup.sh</code> cronjob script</p> <p>When setting up a new clone, first follow the standard deployment steps above, however do not create the <code>binraryData</code> folder</p> <p>You'll first want to restore the Docker volumes -- contained in the <code>webknosos_backups/</code> S3 subdirectory for wherever your cron jobs send the compressed backups</p> <p>Copy the appropriate assets from S3 to the EC2 instance via the <code>aws cp &lt;backup-bucket&gt; &lt;current destination&gt;</code></p> <p>For example:</p> <pre><code>aws s3 cp s3://linc-brain-mit-staging-us-east-2/fossildb_backups/backup_2024-08-20_02-00-02.tar.gz ./backup_2024-08-20_02-00-02.tar.gz\n</code></pre> <p>Once you decompress (can use a tool like <code>gunzip</code>) and then extract the files -- (e.g. <code>tar -cvzf /home/ec2-user/opt/webknossos/webknossos_backup.tar.gz .</code>) you are ready to proceed; however, ensure that <code>persistent</code> and <code>binaryData</code> folders from the extracted files are in the same directory as your <code>docker-compose.yml</code> file</p> <p>Next, you want to restore the <code>fossildb</code> instance -- this can simply be done via the <code>docker-compose run fossil-db-restore</code> command</p> <p>Almost there! You'll next want to bring up the remainder of the WEBKNOSSOS API (along with the nginx-proxy, postgres, etc.) via <code>docker-compose --env-file env.txt webknossos nginx-proxy</code></p> <p>Notably, this will bring up the <code>postgres</code> container (however, we've yet to restore the container!). Thus you'll want to:   - Mount the decompressed, unpacked backup (should be something like <code>&lt;backup_timestamp&gt;.sql</code>). The mount command should be something similar to: <code>docker cp /local/path/to/postgres_backup.sql &lt;container_id&gt;:/tmp/postgres_backup.sql</code>   - Exec into the <code>postgres</code> container and open a <code>psql</code> shell via <code>psql -U postgres</code>   - Next, drop the <code>webknossos</code> database -- e.g. <code>DROP DATABASE webknossos</code>   - Create the database <code>webknossos</code> -- e.g. <code>CREATE DATABASE webknossos</code>   - Restore the database's state via psql -- e.g. <code>psql -U postgres -d webknossos -f /tmp/webknossos_backup.sql</code></p> <p>Your clone should be all set now!</p>"},{"location":"webknossos-deployment/#renew-ssl-certificate","title":"Renew SSL certificate","text":"<p>Note that if you are renewing the certificate, the application cannot be running so proceed with the following steps:</p> <ol> <li>Notify users</li> <li>Stop the application   <pre><code>sudo docker-compose down\n</code></pre></li> <li>Obtain the SSL certificate   <pre><code>sudo docker run --rm -p 80:80 -v $(pwd)/certs:/etc/letsencrypt -v $(pwd)/certs-data:/data/letsencrypt certbot/certbot certonly --standalone -d webknossos.lincbrain.org --email admin@lincbrain.org --agree-tos --non-interactive\n</code></pre></li> <li>Restart the application   <pre><code>sudo /usr/local/bin/docker-compose --env-file env.txt up -d webknossos nginx-proxy\n</code></pre></li> </ol>"},{"location":"webknossos-development/","title":"LINC | WebKNOSSOS Development","text":"<p>This document is designed to help develop new features locally for LINC | WebKNOSSOS</p>"},{"location":"webknossos-development/#cli-installation","title":"CLI Installation","text":"<p>WebKNOSSOS CLI tool docs say that there is support between Python&gt;=3.8,&lt;=3.11.  There were some intractable failures with 3.11, so 3.10 is used here</p> <p>First, create a venv within 3.11 -- pyenv is a good tool to use if you are using 3.12 locally</p> <p>Note: if you are on a Mac, you'll need to ensure you are emulating <code>x86_64</code> arch locally -- a quick fix for this is to prefix <code>arch -x86_64</code> with each command -- e.g. <code>arch -x86_64 pyenv exec python -m venv venv</code></p> <pre><code># Set up local environment\npyenv local 3.11\npyenv exec python3 -m venv venv\nsource venv/bin/activate\n\n# Install webknossos library\npip install webknossos\n\n# Failure might occur with finding PIL import, thus possibly:\npip install Pillow\n\n# At this point, you should be able to use the webknossos CLI --\nwebknossos help\n</code></pre> <p>You'll need to set your authentication token as an env var <code>WK_TOKEN</code>: go to https://webknossos.lincbrain.org/auth/token  (or https://webknossos-staging.lincbrain.org/auth/token if you are on staging)</p>"},{"location":"webknossos-development/#temp-links","title":"Temp Links","text":"<p>https://webknossos-staging.lincbrain.org/api/v5/user/annotations</p>"},{"location":"webknossos-export-annotation/","title":"Export Annotations from Webknossos","text":"<p>The following steps provide instructions for downloading annotations from webknossos. </p> <ol> <li> <p>Go to the <code>Annotations</code> tab and click <code>Open</code> for a desired annotation.</p> <p></p> </li> <li> <p>Click <code>Download</code> in the dropdown <code>Menu</code>.</p> <p></p> </li> <li> <p>Select <code>Include volume annotations as WKW</code> and click on <code>Download</code>.</p> <p></p> </li> <li> <p>Unzip the downloaded annotation file.</p> </li> <li> <p>Unzip the <code>data_Volume.zip</code> file which is located in the annotation folder. </p> <p></p> </li> <li> <p>Run the webknossos_annotation.py script to save into the OME-Zarr format following the <code>czyx</code> direction. </p> </li> </ol>"},{"location":"data/hipct/","title":"HiP-CT data","text":"<p>HiP-CT stands for hierarchical phase-contrast tomography. For more information on the modality see the HiP-CT website. Most of the time HiP-CT consists of a single 'low' (but still quite high!) resolution scan of a whole sample, and then one or more 'high' resolution scans of particular volumes of interest within the same sample.</p>"},{"location":"data/hipct/#data-acquisition-and-naming","title":"Data acquisition and naming","text":"<p>There are HiP-CT scans of two donors, S45 and I58. The data are part of the Pilot human brains dataset on lincbrain.</p> <p>Datasets have the name <code>sub-&lt;donor&gt;_sample-&lt;sample name&gt;_chunk&lt;chunk number&gt;_PC.ome.zarr</code>. The chunk number is set to <code>yyxx</code>, where yy is the resolution of the scan in micrometers, and xx is a scan number that is unique between datasets. For example, a chunk number of <code>402</code> is the second 4um scan. The lowest resolution dataset (equivalently, highest chunk number) is the overview dataset that contains an image of the whole sample. There are then one or more datasets at higher resolution.</p>"},{"location":"data/hipct/#registration","title":"Registration","text":"<p>Each high resolution dataset is registered to the overview dataset using <code>hipct-reg</code>. Each transform maps the pixel coordinates of the high resolution zoom datasets to the overview full-sample dataset. The transform is of the form:</p> \\[ T(\\mathbf{x}) = s \\left ( \\mathbf{R} \\cdot\\mathbf{x} \\right ) + \\mathbf{t} \\] <p>Where</p> <ul> <li>\\(s\\) is a scaling factor</li> <li>\\(\\mathbf{R}\\) is a rotation matrix that rotates about the z-axis.</li> <li>\\(\\mathbf{t}\\) is a translation vector</li> </ul> <p>The rotation matrix takes the form</p> \\[ \\mathbf{R} = \\begin{pmatrix} \\cos(-\\theta) &amp; \\sin(-\\theta) &amp; 0 \\\\ -\\sin(-\\theta) &amp; \\cos(-\\theta) &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\\\ \\end{pmatrix} \\] <p>The transform has 5 parameters (1 scaling, 1 rotation, 3 translation).</p>"},{"location":"data/hipct/#registration-accuracy","title":"Registration accuracy","text":"<p>The HiP-CT registration pipeline is designed to be accurate enough to approximately visually align two datasets. It is not designed for pixel perfect registration. We recommend anyone doing analysis relying on the registration between two datasets performs a more accurate registration themselves, using our registration as an initial starting point.</p>"},{"location":"data/hipct/#registration-data","title":"Registration data","text":"Dataset Overview dataset \\(\\mathbf{t}\\) \\(s\\) \\(\\theta\\) (degrees) <code>sub-I58_sample-blockIC2_chunk-101_PC</code> <code>sub-I58_sample-blockIC2_chunk-401_PC</code> (3746.4467746089194, 3660.0610535229666, 1127.3616712249263) 0.2016106611944697 -6.496401244495998 <code>sub-I58_sample-blockIC2_chunk-201_PC</code> <code>sub-I58_sample-blockIC2_chunk-401_PC</code> (2119.695989239648, 2321.6572801771194, 971.0041825720597) 0.5181803632402424 0.5004912711693281"}]}